# DECISION LOG

**Version**: 1.4

**Last Updated**: December 13, 2025

**Purpose**: Record strategic, tactical, and operational decisions with rationale, impact level, and outcomes.

---

## HOW THIS DOCUMENT WORKS

### Update Workflow

**You don't manually fill this log. Instead:**

1. **Weekly Review Conversation** (Every Friday):
    - You: "Here's what I decided this week: [describe decisions]"
    - Claude: Extracts decisions, assigns impact level, formats them, adds DEC numbers
    - Claude: **Shows formatted entries and waits for your approval**
    - You: "Approved - update the log" (or request changes)
    - Claude: **In next response**, adds to document
2. **Monthly Outcome Review** (Last Sunday of month):
    - You: Share this document + "Update outcomes for decisions made 1-3 months ago"
    - Claude: Lists decisions needing review, asks what happened
    - Claude: **Shows updated outcomes and waits for approval**
    - You: "Approved - update the log"
    - Claude: **In next response**, updates document
3. **Quarterly Pattern Analysis** (End of Month 3, 6, 9, 12):
    - You: Share document + "Analyze decision patterns"
    - Claude: Analyzes patterns, focuses on High Impact decisions
    - Claude: **Shows analysis and waits for approval**
    - You: "Approved - update the log"
    - Claude: **In next response**, updates document

**Your job**: Describe decisions in conversation

**Claude's job**: Format, number, assign impact level - but only update after your explicit approval in a new response

---

## CONVERSATION PROMPTS FOR YOU

### Weekly Decision Capture (Use every Friday)

**Copy-paste this to Claude**:

`Here's my Decision Log. This week I made these decisions:

1. [Describe decision 1]
2. [Describe decision 2]
3. [Describe decision 3]

For each decision, please:
- Extract the decision, context, and rationale
- Assign impact level (High/Medium/Low) and DEC number
- Use Mini Format for tactical decisions, Full Format for strategic ones
- Show me the formatted entries
- Wait for my approval before updating the document`

---

### Monthly Outcome Update (Use last Sunday)

**Copy-paste this to Claude**:

`Here's my Decision Log. Please review decisions made 1-3 months ago that need "Actual Outcome" updates.

For each decision that needs review:
- Show me the decision
- I'll tell you what actually happened
- Show me the updated outcome
- Wait for my approval before updating the document

Let's go through them one by one.`

---

### Quarterly Pattern Analysis (Use end of M3/6/9/12)

**Copy-paste this to Claude**:

`Here's my Decision Log. We're at end of [Month X].

Please:
1. Analyze decisions from the past 3 months
2. Focus on High Impact decisions and their outcomes
3. Identify patterns (what worked, what didn't)
4. Extract key lessons
5. Show me the "[Month X-Y] Pattern Recognition" section
6. Wait for my approval before updating the document`

---

### Filter High Impact Decisions (Use anytime)

**Copy-paste this to Claude**:

`Show me all High Impact decisions from [time period / all time].

For each one, show:
- DEC number and title
- Decision summary
- Current outcome status
- Whether it succeeded, failed, or was modified`

---

## DECISION FORMATS

### Full Format (Strategic Decisions - 10-15 mins to capture)

**When to use**: Decisions that change strategy, business model, value proposition, or core product features

**Claude will format like this**:

markdown

`### DEC-XXX: [Decision Title] (YYYY-MM-DD)

****Decision****: [What you decided]

****Context****: [Situation that prompted this]

****Rationale****: [Why you chose this path]

****Alternatives Considered****:
- Option A: [Description] - Rejected because [reason]
- Option B: [Description] - Rejected because [reason]

****Expected Outcome****: [What you hope will happen]

****Actual Outcome**** (Review Date: YYYY-MM-DD): [To be filled later]

****Impact Level****: High / Medium / Low  
****Tags****: [strategy, product, pricing, content, vendor, technical, workflow, go-to-market]`

---

### Mini Format (Tactical Decisions - 3-5 mins to capture)

**When to use**: Process changes, resource allocation, feature prioritization, tactical adjustments

**Claude will format like this**:

markdown

`### DEC-XXX: [Decision Title] (YYYY-MM-DD)

****Decision****: [One sentence]

****Why****: [One paragraph reasoning]

****Expected Outcome****: [One sentence]

****Review Date****: YYYY-MM-DD  
****Actual Outcome****: [To be filled later]

****Impact Level****: High / Medium / Low  
****Tags****: [relevant tags]
```

---

## DECISION CLASSIFICATION GUIDE (FOR CLAUDE)

### Format Selection: Full vs Mini

**Simple rule**: Does this decision change **what** you're doing (strategy, business model, value prop, core features)? → **Full Format**  
Does it change **how** you execute existing strategy? → **Mini Format**

**Full Format** (Strategic):
- Pricing or business model changes
- Target market, ICP, or positioning changes
- Core product features that define value prop
- Technology platform decisions
- Vertical expansion or market entry
- Major pivots or strategic reversals

**Mini Format** (Tactical):
- Process, workflow, or operational changes
- Resource allocation or time management
- Feature prioritization within existing roadmap
- Tool or platform selection within existing stack
- Content approach or social media strategy
- Vendor outreach tactics

**When unsure**: Ask user "Should this be Full or Mini format?"

---

### Impact Level Assignment

**Claude assigns Impact Level automatically based on this framework:**

**High Impact** = Changes foundational business elements:
- **Business model**: Pricing structure, revenue model, monetization approach
- **Market position**: ICP, target geography, competitive positioning
- **Core value prop**: Features that define "what we sell" (Request Board, Editorial Picks)
- **Platform decisions**: Technology choices that are costly to reverse (Softr → custom)
- **Strategic direction**: Vertical selection, market entry/exit, major pivots

**Medium Impact** = Changes execution approach with measurable consequences:
- **Workflow/process**: How content is produced, vendor outreach sequencing
- **Resource allocation**: Time split, budget priorities, hiring decisions
- **Tactical approach**: Email templates, content types, social media strategy
- **Tool selection**: Choosing between Notion/Airtable within existing workflow
- **Feature scope**: Building X before Y within existing product vision

**Low Impact** = Operational adjustments, easily reversible:
- **Routine optimization**: Page load speed, image compression
- **Cosmetic changes**: UI colors, button text, layout tweaks
- **Administrative**: Meeting schedules, documentation formatting
- **Minor tactical**: Posting frequency, email send times

**Key test**: "If this decision fails, how much needs to change?"
- Entire strategy/business model = **High**
- Execution approach in one area = **Medium**
- Just this one thing = **Low**

**User can override**: If user specifies different impact level, always use their judgment.

---

### Standard Tags (Use these, don't create new ones)

**Core tags** (use most frequently):
- `strategy` - Strategic direction, positioning, market focus
- `product` - Product features, roadmap, user experience
- `pricing` - Pricing model, costs, revenue structure
- `business-model` - Monetization, revenue streams, unit economics
- `content` - Content production, SEO, article strategy
- `vendor` - Vendor acquisition, retention, relationships
- `technical` - Technology stack, platforms, infrastructure
- `workflow` - Processes, operations, team workflows
- `go-to-market` - Marketing, sales, distribution channels
- `ICP` - Target customer, market segmentation
- `positioning` - Brand, differentiation, messaging
- `vertical-expansion` - Adding new verticals (L&D, CS)
- `focus` - Prioritization, saying no, resource constraints
- `speed-to-market` - Velocity, launch timing, validation speed
- `MVP` - Minimum viable product, lean approach
- `differentiation` - Competitive moat, unique value
- `monetization` - Revenue generation, pricing
- `early-adopters` - First customers, validation cohort
- `productivity` - Efficiency, time savings, automation
- `AI-assisted` - Using AI tools in workflow
- `market` - Geographic markets, location strategy
- `geography` - Physical location, timezone, currency

**If decision needs a tag not on this list**: Use the closest match, or ask user if new tag is needed.

---

## CURRENT DECISIONS

---

### FOUNDATIONAL DECISIONS (Pre-Launch Phase - November 2025)

The following 10 decisions document core strategic choices made during the planning phase, before execution began. They were consolidated from Master Plan v1.0 and represent the foundational assumptions for Year 1.

These decisions all use **Full Format** and are all **High Impact** because they define core strategy, business model, and product approach.`

## **DEC-001 (UPDATED): Build custom platform using Next.js instead of Softr**

**Date**: 2025-11-19

**Impact**: High

**Tags**: technical, strategy, speed-to-market, product

### **Decision**

Build the MVP using **Next.js 14 + React + Tailwind + Airtable**, instead of Softr.

### **Context**

Original plan (Nov 16) was to use Softr for rapid no-code launch.

After technical review, it became clear:

- You want to learn React long-term
- Next.js is better for SEO
- Airtable + Next.js gives full control over data
- The system isn't actually 8–10 weeks of dev (3–4 weeks suffices with AI help)

### **Rationale**

- **SEO**: Next.js server-side rendering → better indexing than Softr
- **Control**: Full custom routing, custom UX, custom components
- **Longevity**: You grow into React/Next.js as your career skill
- **Faster iteration**: React components more flexible than Softr blocks
- **Cost**: No Softr fees; Vercel free tier covers early months

### **Alternatives Considered**

- **Stay with Softr**
    
    Rejected due to poor SEO, limited control, and learning opportunity loss.
    
- **Pure React SPA**
    
    Rejected due to zero SEO and no SSR.
    
- **WordPress + plugins**
    
    Rejected due to heavy, fragile CMS ecosystem.
    

### **Expected Outcome**

- SEO-optimized content pages (/articles/[slug])
- Fully custom UI with shadcn/ui
- Faster long-term roadmap
- Better ability to scale features (Vendor Portal, analytics, etc.)

### **Actual Outcome** (Review Date: 2026-02-15)

[To be filled later]

`### DEC-002: Organize by industry verticals instead of generic categories (2025-11-16)

**Summary**: Vertical-first positioning (HR, L&D, CS) differentiates from horizontal competitors and enables better buyer matching.

**Decision**: Structure marketplace around industry verticals (HR, L&D, CS) rather than horizontal categories (productivity, analytics, communication)

**Context**:
- Competitive landscape dominated by generic tools (Futurepedia lists 5000+ tools across all categories)
- Buyers are overwhelmed by choice and lack context
- Enterprise buyers think in terms of "HR tools" not "productivity tools"

**Rationale**:
- **Differentiation**: Every competitor uses horizontal categories. Vertical-first is our unique positioning
- **Buyer psychology**: HR leaders search for "AI HR tools," not "AI productivity tools for various departments"
- **Better matching**: Industry context allows better Request Board matching (company size, compliance needs, use cases are industry-specific)
- **Content depth**: Can write "Best AI Tools for HR" with real expertise vs shallow "Best AI Productivity Tools" covering 10 industries
- **Network effects**: HR vendors want to be listed alongside other HR vendors, creating category cohesion

**Alternatives Considered**:
- **Horizontal categories** (Productivity, Analytics, Communication): Standard approach, easier to scale initially. Rejected because no differentiation from competitors
- **Hybrid** (both vertical and horizontal): Confusing navigation, dilutes positioning. Rejected because clarity > comprehensiveness
- **Job title focus** ("Tools for CHROs"): Too narrow, limits scalability. Rejected because verticals are broader and easier to expand

**Expected Outcome**:
- Clearer positioning in market ("The HR AI Tool Marketplace")
- Higher quality content (depth > breadth)
- Better Request Board matches (industry-specific context)
- Easier vendor sales pitch ("We're the HR tool marketplace" vs "We list all AI tools")

**Trade-offs Accepted**:
- Slower initial growth (focusing on one vertical vs launching with many)
- Can't list every tool immediately (only those relevant to HR)
- Must learn each vertical deeply before expanding

**Actual Outcome** (Review Date: 2026-02-15):
[To be filled during monthly review]

**Impact Level**: High  
**Tags**: strategy, positioning, product

---

### DEC-003: Target 50-500 employee companies (mid-market) (2025-11-16)

**Summary**: Mid-market focus balances budget availability, decision speed, and tool fit better than SMB or Enterprise.

**Decision**: Focus on mid-market companies (50-500 employees) rather than SMB (1-50) or Enterprise (500+)

**Context**:
- Need clear ICP for content, Request Board matching, and vendor targeting
- Different segments have vastly different needs, budgets, and buying processes
- Can't serve everyone well initially

**Rationale**:
- **Budget availability**: Mid-market has budget for tools (£500-5000/mo range), unlike SMBs who need free/cheap options
- **Decision speed**: Faster than enterprise (weeks vs months), important for validation timeline
- **Pain intensity**: Growing fast, feeling pain of manual processes, ready to invest in solutions
- **Tool fit**: Most AI tools target this segment (SMB tools too simple, enterprise tools too complex/expensive)
- **Vendor overlap**: Vendors want to reach this segment (high conversion, reasonable sales cycles)

**Alternatives Considered**:
- **SMB focus (1-50 employees)**: Larger addressable market but lower budgets. Rejected because free-tier tools dominate, harder to monetize
- **Enterprise focus (500+)**: Higher deal values but slower sales cycles, complex requirements. Rejected because too slow for Year 1 validation
- **Broad (all segments)**: Diluted messaging, can't match well in Request Board. Rejected because need focus to execute well

**Expected Outcome**:
- Clear content targeting ("If you're an HR leader at a 200-person company...")
- Better tool recommendations (filter out SMB-only and enterprise-only tools)
- Vendors excited about ICP overlap
- Request Board matches lead to higher conversion (tools actually fit buyer needs)

**Trade-offs Accepted**:
- Smaller addressable market initially
- May need to reject some Request Board submissions ("Sorry, we focus on 50-500 employee companies")
- Some great tools excluded because they only serve SMB or Enterprise

**Actual Outcome** (Review Date: 2026-02-15):
[To be filled during monthly review]

**Impact Level**: High  
**Tags**: strategy, ICP

---

### DEC-004: Start with HR vertical before L&D and CS (2025-11-16)

**Summary**: HR-first with proven playbook for L&D (M4-5) and CS (M7-9) beats launching three mediocre verticals simultaneously.

**Decision**: Launch with HR & Talent vertical only, add L&D in Month 4-5, CS in Month 7-9

**Context**:
- Need to focus limited time (30-36 hrs/week)
- Could attempt to launch all three verticals simultaneously
- Risk of doing three things poorly vs one thing well

**Rationale**:
- **Focus**: 30-36 hours/week means depth > breadth. Better to dominate HR than be mediocre in three verticals
- **Proven template**: Launching HR successfully creates playbook for L&D and CS (same vendor outreach process, same Request Board logic, same content templates)
- **Vendor overlap**: ~70% of HR vendors also offer L&D tools → easier to upsell existing relationships when adding L&D
- **Content reuse**: Comparison article template works across all verticals, just swap tools and terminology
- **Lower risk**: If HR fails, knowing that in Month 3 is better than discovering all three verticals fail in Month 6

**Alternatives Considered**:
- **Launch all three simultaneously**: Faster coverage but 3x the work with same time budget. Rejected because quality would suffer across all three
- **Start with L&D**: Smaller market than HR. Rejected because HR has more tools, more buyers, more validation signal
- **Start with CS**: Crowded market (Zendesk, Intercom dominate). Rejected because harder differentiation

**Expected Outcome**:
- HR vertical fully built by Week 12 (20-30 tools, 5-8 articles, Request Board functional)
- 5-8 paying HR vendors by Month 3 (validates business model)
- HR playbook documented, ready to replicate for L&D
- Month 4-5: Add L&D in 3-4 weeks (using HR playbook)
- Month 7-9: Add CS in 3-4 weeks

**Trade-offs Accepted**:
- Slower initial growth (can't capture L&D/CS buyers immediately)
- Some vendors might say "call us when you have L&D" (acceptable, come back later)
- Competitors might launch multi-vertical before we do (acceptable, we'll have better depth)

**Actual Outcome** (Review Date: 2026-02-15):
[To be filled during monthly review]

**Impact Level**: High  
**Tags**: strategy, vertical-expansion, focus

---

### DEC-005: Premium pricing at £99/month per vertical (2025-11-16)

**Summary**: £99/mo per vertical (not per tool, not per company) balances affordability with expansion revenue.

**Decision**: Premium vendor tier priced at £99/month per vertical (not per tool, not per company)

**Context**:
- Need to determine pricing model before vendor outreach begins
- Competitors charge anywhere from £200-2000/month
- Early-stage platform with zero traction

**Rationale**:
- **Per-vertical pricing aligns incentives**: Vendor pays for each vertical they want featured in (HR, L&D, CS). If they have tools in multiple verticals, they pay multiple subscriptions
- **Affordable barrier to entry**: £99/mo is impulse-buy territory for mid-market vendors. Don't need exec approval, marketing manager can decide
- **Easy expansion**: When we add L&D vertical, existing HR vendors can add L&D subscription for additional £99/mo (£198/mo total)
- **Competitive positioning**: Lower than G2/Capterra (£500-2000/mo) but premium enough to be taken seriously
- **Psychological threshold**: £99 sits just under £100 psychological barrier. £120 feels more expensive despite being only 21% more

**Alternatives Considered**:
- **£49/mo**: Too cheap, vendors won't take it seriously, implies low-value platform. Rejected because positioning matters
- **£199/mo**: Cleaner number but requires more traction to justify. Rejected because too high for zero-traction platform
- **Per-tool pricing** (£99 per tool): Vendors with multiple tools pay more. Rejected because penalizes vendors with robust product suites
- **Flat £99 for all verticals**: Lost expansion revenue. Rejected because leaves money on the table

**Expected Outcome**:
- 60-70% of approached vendors say "yes" or "maybe" to £99/mo (vs 30-40% at £199/mo)
- Clear expansion path: HR-only vendor (£99) → HR+L&D vendor (£198) → HR+L&D+CS vendor (£297)
- Target Month 3: 5-8 premium vendors = £495-792 MRR
- Can always raise prices later once traction is proven (grandfather early vendors)

**Trade-offs Accepted**:
- Lower revenue per vendor than competitors (but higher volume potential)
- Need more vendors to hit same MRR as charging £299/mo
- Might be perceived as "budget option" (mitigated by quality of platform)

**Actual Outcome** (Review Date: 2026-02-15):
[To be filled during monthly review]

**Impact Level**: High  
**Tags**: pricing, business-model, monetization

---

### DEC-006: Early Bird offer - 3 months free for first 10 vendors (2025-11-16)

**Summary**: 3 months free (not 1 month, not 50% off) gives time to prove value while filtering tire-kickers.

**Decision**: First 10 Premium vendors receive 3 months free, then £99/mo (not 50% discount, not 1 month free)

**Context**:
- Zero traction, zero social proof, zero vendor testimonials
- Need early partners to validate platform and create case studies
- Vendors understandably skeptical of brand-new marketplace

**Rationale**:
- **Risk-free trial**: 3 months is long enough to see real results (content published, Request Board submissions received, leads delivered)
- **Commitment signal**: Not free forever - vendors who sign up expect to pay eventually, filters out tire-kickers
- **Urgency**: "First 10 only" creates FOMO and urgency to commit now
- **Case study generation**: 3 months gives us time to gather success stories and metrics to show later vendors
- **Fair exchange**: Vendors get free trial, we get early validation and testimonials

**Alternatives Considered**:
- **50% discount forever**: Leaves money on table indefinitely. Rejected because devalues product long-term
- **1 month free**: Not enough time to see results, vendors won't see value. Rejected because too short
- **Free forever for first 10**: No revenue from early partners. Rejected because need revenue signal for validation
- **No discount**: Hard to get first customers with zero proof. Rejected because too high-friction for cold start

**Expected Outcome**:
- 8-10 vendors sign up in Month 1 (capitalizing on Early Bird offer)
- Month 4: First wave of renewals (6-8 vendors convert to paying)
- Case studies from Early Bird cohort used to sell next 20 vendors
- If only 3-4 vendors sign up in Month 1, extend offer or pivot strategy

**Trade-offs Accepted**:
- Zero revenue for first 3 months (acceptable, validation > short-term revenue)
- Some vendors might churn after free period (acceptable, need true believers)
- Later vendors might ask for same deal (say no, Early Bird expired)

**Actual Outcome** (Review Date: 2026-02-15):
[To be filled during monthly review]

**Impact Level**: High  
**Tags**: pricing, go-to-market, early-adopters

---

### DEC-007: AI-assisted content production, not fully manual (2025-11-16)

**Summary**: AI draft (1hr) + human review (1.5hr) = 2.5hr vs 6-8hr manual, enabling 3-4 articles/week vs 1/week.

**Decision**: Use Claude/ChatGPT to generate first drafts, then human review/edit rather than writing from scratch

**Context**:
- Need 8-10 articles for SEO by end of Month 3
- Writing high-quality articles manually takes 6-8 hours each
- 30-36 hours/week total time budget (content is only one workstream)

**Rationale**:
- **Time efficiency**: AI draft (1 hour) + human review (1.5 hours) = 2.5 hours vs 6-8 hours manual. 60-70% time savings
- **Quality control**: Human review catches AI hallucinations, adds personal insights, ensures accuracy
- **Consistency**: AI maintains consistent structure and tone across articles
- **Scalability**: Can produce 3-4 articles/week with AI vs 1 article/week manual
- **Focus on high-value work**: Spend saved time on vendor outreach (higher ROI)

**Alternatives Considered**:
- **Fully manual writing**: Highest quality but too slow. Rejected because can't produce enough content for SEO
- **Fully AI with no review**: Fast but risky (hallucinations, inaccuracies, generic content). Rejected because quality matters
- **Hire freelance writers**: £100-300 per article. Rejected because (a) expensive, (b) need domain expertise, (c) learning curve for each writer
- **No content strategy**: Focus only on Request Board and vendor outreach. Rejected because SEO is critical long-term traffic source

**Expected Outcome**:
- Publish 8-10 articles by Month 3 (vs 3-4 if fully manual)
- Articles are 80% as good as fully manual (acceptable trade-off)
- Time saved used for vendor outreach (higher-leverage activity)
- SEO traffic starts appearing Month 4-6

**Quality Checklist for AI Content**:
- Always fact-check tool features, pricing, compliance claims
- Add Request Board insights (patterns from real submissions)
- Rewrite intro and conclusion (most important for reader)
- Verify no AI hallucinations or outdated info
- Add 3+ internal links, 2+ external links

**Trade-offs Accepted**:
- Articles slightly more generic than if fully manual (mitigated by human review)
- Need to develop AI prompting skills (investment upfront)
- Some sections might need heavy rewriting

**Actual Outcome** (Review Date: 2026-02-15):
[To be filled during monthly review]

**Impact Level**: High  
**Tags**: content, productivity, AI-assisted, workflow

---

### DEC-008: Request Board as differentiated feature from Day 1 (2025-11-16)

**Summary**: Request Board (qualified lead matching) is core value prop, not post-launch add-on, differentiating from passive directories.

**Decision**: Launch Request Board as core product feature from Day 1, not added later as enhancement

**Context**:
- Could launch as pure directory (just tool listings)
- Request Board requires additional build time and operational overhead (manual matching initially)
- Most directories don't have buyer-to-vendor matching

**Rationale**:
- **Core differentiation**: "Qualified leads, not just passive listings" is the entire vendor value prop. Without Request Board, we're just another directory
- **Two-sided value**: Buyers get personalized recommendations, vendors get warm leads with context
- **Competitive moat**: G2/Capterra are review platforms, not lead generation. Request Board is fundamentally different
- **Pricing justification**: £99/mo makes sense for lead gen, harder to justify for passive listing
- **Data goldmine**: Request Board submissions reveal buyer pain points → informs content strategy, tool gaps, vendor needs

**Alternatives Considered**:
- **Directory-first, Request Board later**: Lower build complexity initially. Rejected because removes core differentiation
- **Request Board only, no directory**: Too narrow, needs tool database for matching. Rejected because buyers need to browse too
- **Charge extra for Request Board leads**: Complicates pricing. Rejected because leads should be included in Premium (all-in-one value prop)

**Expected Outcome**:
- Request Board is primary sales pitch in vendor outreach ("We send you qualified leads")
- 10-15 Request Board submissions by Month 3
- 70-80% of submissions result in successful vendor matches
- Request Board data informs content strategy (write articles about common requests)

**Operational Plan**:
- Month 0-3: Manual matching (Nelson reviews each submission, selects 2-3 tools, notifies vendors)
- Month 3-6: Semi-automated (Airtable formulas suggest matches, Nelson reviews)
- Month 6+: Mostly automated (only edge cases require manual review)

**Trade-offs Accepted**:
- Extra build time for Request Board form and workflow (1-2 weeks)
- Manual matching overhead initially (30 mins per submission)
- Risk that buyers don't use it (mitigated by making it prominent and easy)

**Actual Outcome** (Review Date: 2026-02-15):
[To be filled during monthly review]

**Impact Level**: High  
**Tags**: product, differentiation, strategy

---

### DEC-009: Three-tier listing model (Free, Premium, Editorial Picks) (2025-11-16)

**Summary**: Free (comprehensive), Premium (monetization), Editorial Picks (quality signal) balance coverage, revenue, and trust.

**Decision**: Three listing tiers - Free (all tools listed), Premium (enhanced with lead notifications), Editorial Picks (hand-curated best tools)

**Context**:
- Could do two-tier (Free vs Premium only)
- Could charge for basic listing (no free tier)
- Need balance between open listings and curated quality

**Rationale**:
- **Free tier = comprehensive coverage**: List all relevant tools for SEO and buyer trust (buyers want options)
- **Premium tier = vendor monetization**: £99/mo for enhanced features and Request Board leads
- **Editorial Picks = quality signal**: Hand-curated "best in category" builds buyer trust and creates aspiration for vendors
- **Clear upgrade path**: Free → Premium → Editorial Pick recognition

**Editorial Picks Criteria**:
- Verified listing (vendor-confirmed information)
- Strong user reviews/reputation
- Clear product-market fit for mid-market (50-500 employees)
- Responsive vendor (answers Request Board leads quickly)
- Not paid placement (editorial integrity)

**Alternatives Considered**:
- **Two-tier only (Free + Premium)**: Simpler but no quality signal. Rejected because buyers need curation
- **Charge for basic listing**: Excludes many tools, worse for SEO. Rejected because need comprehensive coverage
- **Premium = automatic Editorial Pick**: Removes editorial integrity. Rejected because quality matters
- **No free tier**: Vendors pay for any listing. Rejected because limits coverage and SEO

**Expected Outcome**:
- Free tier: 50-80 tools by Month 3 (comprehensive coverage)
- Premium tier: 5-8 vendors by Month 3 (paying customers)
- Editorial Picks: 10-15 tools by Month 3 (top 2-3 per category)
- Editorial Picks become aspiration for Free tier vendors ("How do I get featured?")

**Trade-offs Accepted**:
- Editorial Picks create extra curation work (acceptable, builds trust)
- Some vendors might be upset about not being Editorial Pick (handle with transparency about criteria)
- Free tier doesn't generate revenue directly (but helps SEO and trust)

**Actual Outcome** (Review Date: 2026-02-15):
[To be filled during monthly review]

**Impact Level**: High  
**Tags**: business-model, product, monetization

---

### DEC-010: London, England as operating base (timezone, currency, market focus) (2025-11-16)

**Summary**: UK base with GBP pricing and GDPR-first positioning differentiates from US-focused competitors.

**Decision**: Operate from London (GMT timezone), price in GBP (£), with initial focus on UK and English-speaking markets

**Context**:
- Currently based in London
- Could price in USD and target US primarily
- Timezone affects vendor outreach and support responsiveness

**Rationale**:
- **UK market underserved**: Most AI tool marketplaces are US-focused (prices in USD, US compliance focus)
- **GDPR-first positioning**: UK/EU buyers care deeply about GDPR compliance. Being UK-based signals credibility
- **Timezone alignment**: GMT allows good overlap with both US East Coast (afternoon calls) and EU (full working day)
- **Currency clarity**: GBP pricing is transparent for UK/EU buyers (no conversion fees/confusion)
- **English-speaking market**: UK, EU, Australia, Canada all accessible

**Alternatives Considered**:
- **USD pricing, US-first**: Larger market but more competitive. Rejected because harder to differentiate
- **Dual pricing (GBP + USD)**: Operational complexity, currency fluctuation issues. Rejected because adds confusion
- **EU focus (EUR pricing)**: Smaller English-speaking market in EU. Rejected because UK market is larger
- **Remote-first, no location**: Works but loses UK market credibility. Rejected because location matters for trust

**Expected Outcome**:
- UK vendors respond better (local credibility)
- GDPR-compliant positioning resonates with UK/EU buyers
- Can serve US vendors too (most have UK presence or are Global)
- Timezone works for both continents

**Vendor Market Distribution** (expected):
- 40-50% Global (serve all markets)
- 25-30% UK-specific
- 15-20% US-specific (but many serve UK too)
- 10-15% EU-specific

**Trade-offs Accepted**:
- Smaller primary market than US (acceptable, less competition)
- Some US vendors might prefer USD pricing (acceptable, can convert)
- Late night calls with West Coast US vendors (manageable)

**Actual Outcome** (Review Date: 2026-02-15):
[To be filled during monthly review]

**Impact Level**: High  
**Tags**: market, geography, positioning

---

## EXECUTION PHASE DECISIONS (Month 1+)

*New decisions will be added here as they're made, in reverse chronological order*

---

## DECISION PATTERNS & LEARNINGS

### Month 1-3 Pattern Recognition

**Review Date**: 2026-02-15

*[Claude will fill this during quarterly review based on your input]*

**High Impact Decisions This Quarter**:
- [List of High Impact decisions made]

**What worked**:
- [Decisions that proved correct]

**What didn't work**:
- [Decisions that needed reversal or adjustment]

**Key lessons**:
- [Insights for future decisions]

---

### Month 4-6 Pattern Recognition

**Review Date**: 2026-05-15

*[To be filled during quarterly review]*

---

### Month 7-12 Pattern Recognition

**Review Date**: 2026-11-15

*[To be filled during quarterly review]*

---

## DECISIONS REVERSED OR MODIFIED

*[This section tracks decisions that were changed and why]*

### Template for Reversed Decisions:

**DEC-XXX REVERSED: [Original Decision] (Reversal Date)**

**Original Decision**: [What was decided]

**Original Impact Level**: High / Medium / Low

**Why Reversed**: [What changed, what data emerged]

**New Decision**: [Current approach]

**New Impact Level**: High / Medium / Low

**Learning**: [What this taught us]

---

## DECISIONS UNDER CONSIDERATION

*[Decisions being actively debated but not yet made]*

### Template for Pending Decisions:

**PENDING-XXX: [Decision Topic]**

**Context**: [What's the situation]

**Options Being Considered**:
- Option A: [Pros and cons]
- Option B: [Pros and cons]

**Potential Impact**: High / Medium / Low

**Data Needed to Decide**: [What info would resolve this]

**Decision Deadline**: [When must this be decided]

**Current Lean**: [Which way you're leaning and why]

---`

`## CHANGELOG

**Version 1.0** (Nov 16, 2025):
- Initial Decision Log created
- Documented 10 foundational decisions made during planning phase
- Established decision entry template
- Created structure for pattern tracking and reversals

**Version 1.1** (Nov 16, 2025):
- Redesigned for conversation-driven updates
- Added "How This Document Works" section with update workflows
- Added conversation prompts for weekly/monthly/quarterly reviews
- Added Mini Format for tactical decisions
- Added one-line summaries for all foundational decisions
- Clarified format selection guide for Claude
- Added pre-launch section marker for foundational decisions

**Version 1.2** (Nov 16, 2025):
- Added Impact Level field (High/Medium/Low) to both Full and Mini formats
- Added comprehensive Impact Level assignment guide for Claude
- Added filter prompt for viewing High Impact decisions
- Updated Pattern Recognition sections to focus on High Impact decisions
- Added Impact Level to reversed/pending decision templates
- Impact Level auto-assigned by Claude, user can override

**Version 1.3** (Nov 16, 2025):
- **Fixed Impact Level definition overlap** - Clarified High/Medium/Low with clear decision test
- **Improved Mini Format readability** - Changed to multi-line format matching Full Format
- **Added explicit approval workflow** - Claude must wait for "Approved - update the log" before making changes
- **Consolidated Impact Level guides** - Single clear framework instead of duplicate sections
- **Added DEC numbering rules** - Numbers are permanent, never reused or reordered
- **Simplified format selection** - One-line rule: strategy change = Full, execution change = Mini
- **Standardized tag list** - Fixed set of tags to prevent Claude creating random new ones`

# **IMPACT LEVEL GUIDE**

Updated examples to reflect Next.js as the new core platform.

### High Impact

Examples:

- Platform decisions (Next.js vs Softr vs WordPress)
- Pricing structure
- Market positioning
- Vertical selection
- Core features (Request Board, Editorial Picks)

### Medium Impact

Examples:

- Content workflow changes
- Outreach cadence changes
- Email templates
- Building Feature A before Feature B

### Low Impact

Examples:

- UI color changes
- Image optimization
- Scheduling routines

# **INSTRUCTIONS FOR CLAUDE (UPDATED)**

Added to the “Core Workflow Rules” section:

### **Platform decisions are ALWAYS High Impact**

Because they fundamentally change:

- SEO
- Architecture
- Development workflow
- Roadmap pacing
- Feature flexibility
- Vendor Portal feasibility

This is now explicitly stated for clarity.

`### When User Requests Decision Capture

**User says**: "Here are decisions I made this week: [descriptions]"

**Your process**:

**Step 1 - Extract and Format** (First response):
1. Extract each decision from user's description
2. Determine format: Strategy/business model/value prop change → Full Format. Execution change → Mini Format
3. Assign Impact Level using the decision test: "If this fails, what needs to change?" (Everything=High, Some things=Medium, Just this=Low)
4. Assign next DEC number: 
   - Check highest existing DEC number in document
   - Add 1 to get next number
   - **DEC numbers are permanent** - never reuse or reorder
5. Format each decision using appropriate template
6. Use tags from standard tag list only
7. Show formatted entries to user
8. Explain your reasoning for format and impact level choices
9. **End with**: "Do you approve these entries? If yes, reply 'Approved - update the log' and I'll add them in my next response."

**Step 2 - Update Document** (Second response, only after approval):
1. User says "Approved - update the log" (or similar)
2. Add formatted decisions under "EXECUTION PHASE DECISIONS" section
3. Use reverse chronological order (newest DEC number first)
4. Confirm: "Added DEC-XXX, DEC-YYY to Decision Log."

**Example**:
```
User: "This week I decided to pause paid ads until Month 6, 
and I also decided to change pricing to £149/mo instead of £99."

You (First Response): 
"I see two decisions:

1. Pause paid ads → Mini Format, Medium Impact
   - This is execution/resource allocation, not strategy change
   - If this fails, you just restart ads - only this one thing changes

2. Pricing change £99 → £149 → Full Format, High Impact
   - This changes core business model and vendor acquisition
   - If this fails, entire monetization approach needs rethinking

Here are the formatted entries:

### DEC-011: Pause paid advertising until Month 6 (2025-11-23)

****Decision****: Pause all paid advertising spend until Month 6 to focus budget on organic channels.

****Why****: Current paid ad ROI is unclear with limited conversion data. Better to invest time in SEO and vendor outreach where we can measure results directly. Will revisit paid ads once we have 10+ Premium vendors and clear conversion metrics.

****Expected Outcome****: £300-500/month saved, redirected to Softr/tooling. No measurable traffic loss since paid ads currently <5% of visitors.

****Review Date****: 2026-05-01  
****Actual Outcome****: [To be filled later]

****Impact Level****: Medium  
****Tags****: go-to-market, workflow

---

### DEC-012: Increase Premium pricing from £99/mo to £149/mo (2025-11-23)

****Decision****: Raise Premium tier pricing from £99/month to £149/month per vertical, effective for all new vendors starting December 1, 2025. Grandfather existing Early Bird vendors at £99/mo.

****Context****: [You'll need to provide context - why the change? What prompted this?]

****Rationale****: [You'll need to provide rationale]

****Alternatives Considered****:
- Keep £99/mo: [Why rejected?]
- Go to £199/mo: [Why rejected?]

****Expected Outcome****: [What do you hope happens?]

****Actual Outcome**** (Review Date: 2026-02-23): [To be filled later]

****Impact Level****: High
****Tags****: pricing, business-model

---

### DEC-013: Vendor data architecture expansion (Schema v2.3) (2025-11-27)

****Decision****: Expand Airtable schema from v2.2 (9 tables, ~180 fields) to v2.3 (12 tables, ~250 fields, 14 relationships) with comprehensive vendor data architecture for pricing transparency, integration quality tracking, compliance verification, implementation timelines, and AI-powered buyer matching.

****Context****: Foundation Phase complete with Next.js + Airtable integration working. Ready to enhance matching algorithm and Premium tier value proposition. Current schema lacks detailed vendor data needed for sophisticated buyer matching and differentiation from generic directories.

****Rationale**:
- **Buyer value**: Enables budget-aware matching (pricing_annual_min/max), compliance filtering (GDPR, EEOC, SOC2), integration compatibility (30 core platforms tracked), and realistic timeline estimation
- **Vendor value**: Showcases pricing transparency, integration quality (Native/API/Zapier), compliance badges, case studies, and implementation support - justifies £99/mo Premium pricing
- **Competitive moat**: Detailed vendor data collection creates barrier to entry - requires systematic AI research + vendor verification workflow that generic directories can't replicate at scale
- **Data quality**: Three-phase approach (AI Research → Vendor Verification → Airtable Import) balances speed (AI populates 60% of fields) with accuracy (vendor confirms critical data)
- **Scalability**: Two-track integration strategy (30 core in junction table + long-tail in text field) prevents Airtable record limit exhaustion (5K vs 20K records)

****Alternatives Considered****:
- **Wait until Month 4-6**: Rejected - data collection takes time, better to start during Foundation Phase while vendor count is low
- **Manual vendor data only**: Rejected - too slow, creates vendor friction barrier before they see value, limits initial tool coverage
- **AI research only (no vendor verification)**: Rejected - accuracy concerns for compliance and pricing, reduces vendor engagement touchpoint
- **50+ core integrations instead of 30**: Rejected - exceeds Airtable record limits at scale (1,000 tools × 50 = 50K records = 100% of base limit)

****Expected Outcome**:
- All 20 initial tools populated with v2.3 vendor data by end of Month 1
- 10+ vendor verification form submissions (50% response rate on first 20 tools)
- Enhanced buyer matching algorithm operational using pricing, region, compliance, and integration filters
- Premium tier differentiation clearly visible on tool pages (pricing ranges, integration quality badges, implementation timelines)
- Foundation for personalized recommendation engine (RECOMMENDATIONS table structure ready for Month 4+ AI matching)

****Actual Outcome**** (Review Date: 2026-02-27): [To be filled later]

****Impact Level**: High
**Tags**: schema, data-architecture, vendor-data, buyer-matching, competitive-moat

---

### DEC-014: AI-Native Only Positioning (2025-12-13)

**Decision**: IndustryLabs will exclusively list AI-native B2B tools (founded 2020+, AI/ML as core capability), not traditional SaaS with "AI features added"

**Context**:
- Initial research assumed ~87 tools mixing traditional SaaS and AI-native solutions
- Research Task 1 (Dec 2025) identified 120 qualifying AI-native HR tools
- Competitive landscape shows no incumbent owns "AI-native only" positioning
- Generic directories (G2, Capterra) mix legacy software (BambooHR, Workday) with AI-native tools (Dover, Metaview)

**Rationale**:
- **Clear differentiation**: "AI-native only" is defensible positioning vs "comprehensive B2B directory"
- **Buyer clarity**: Buyers know exactly what they're getting (no confusion between legacy + AI bolt-ons vs AI-first architecture)
- **Vendor quality signal**: Founded 2020+ correlates with modern stack, cloud-native, API-first design
- **Market timing**: AI-native tools reached critical mass (~120 in HR alone) to sustain curated marketplace
- **Content depth**: Can write authoritative guides on "best AI-native tools" vs generic "best HR tools"

**Alternatives Considered**:
- **Mix traditional + AI-native**: Rejected - removes differentiation, creates buyer confusion, dilutes positioning
- **AI-enhanced only** (tools with any AI features): Rejected - too broad, includes legacy SaaS with marketing AI claims
- **Founded 2018+** (earlier cutoff): Rejected - includes pre-GPT era tools without modern AI capabilities
- **AI-native + select legacy incumbents**: Rejected - positioning integrity matters more than short-term coverage

**Expected Outcome**:
- Clear marketing message: "The only marketplace exclusively for AI-native People Ops tools"
- Vendor qualification becomes binary (founded 2020+ with AI core = yes, all else = no)
- 60-75 curated tools per vertical (quality over 200+ mediocre listings)
- Competitive moat through editorial standards (not just "list everything")

**Trade-offs Accepted**:
- Excludes some popular traditional tools (Workday, BambooHR, Greenhouse)
- Smaller initial TAM (only ~120 HR tools vs 500+ if including legacy)
- Some buyers might want "all options" (redirect to G2/Capterra)
- Must defend cutoff criteria when vendors ask "why not us?"

**Actual Outcome** (Review Date: 2026-03-13):
[To be filled during monthly review]

**Impact Level**: High
**Tags**: strategy, positioning, differentiation, ICP

---

### DEC-015: Curated 60-75 Tools Strategy (Not Comprehensive Directory) (2025-12-13)

**Decision**: Curate 60-75 rigorously vetted tools per vertical, not 200+ comprehensive listings. Quality over quantity.

**Context**:
- Research Task 2 analyzed 55-tool portfolio with visibility/sales maturity scoring
- Analysis revealed clear tiers: high-visibility + strong sales (Tier 1), niche specialists (Tier 2), low visibility (Tier 3), pre-PMF (Tier 4)
- Generic directories suffer from "too many mediocre options" problem (Futurepedia lists 5,000+ tools)
- Buyers report overwhelm, not lack of options

**Rationale**:
- **Buyer trust**: 60-75 vetted tools signals curation, not exhaustive aggregation
- **Operational feasibility**: Solo founder can maintain quality on 75 tools, not 200+
- **Vendor value**: Premium tier vendors don't want to be buried among 150 competitors in same category
- **SEO strategy**: Better to rank #1 for "best AI recruiting tools (curated)" than #47 for "AI tools database"
- **Tier-based selection**: Focus outreach on Tier 1 (70-85% conversion) and Tier 2 (50-70%), skip Tier 4 (0-15%)

**Alternatives Considered**:
- **Comprehensive 200+ tools**: Rejected - quality suffers, buyers overwhelmed, vendor value diluted
- **Ultra-exclusive 20-30 tools**: Rejected - too narrow, limits buyer choice, harder to cover all use cases
- **No curation, list everything**: Rejected - commoditizes marketplace, removes differentiation

**Expected Outcome**:
- 60-75 tools per vertical by Month 12 (vs 20-30 in comprehensive directory approach)
- Clear editorial standards: Tier 1-2 prioritized, Tier 3 case-by-case, Tier 4 excluded
- Vendor pitch: "We're selective - only 60-75 tools per vertical, not 200+ mediocre listings"
- Buyer pitch: "We did the research - here are the 60-75 tools actually worth evaluating"

**Trade-offs Accepted**:
- Some tools excluded despite being "okay" (acceptable - curation requires hard choices)
- Vendors might complain about not being listed (acceptable - editorial integrity matters)
- Slightly smaller SEO surface area than comprehensive directory (acceptable - quality traffic > volume)

**Actual Outcome** (Review Date: 2026-03-13):
[To be filled during monthly review]

**Impact Level**: High
**Tags**: strategy, product, curation, differentiation

---

### DEC-016: Tier-Based Financial Modeling (Conservative/Recommended/Aggressive) (2025-12-13)

**Decision**: Vendor acquisition and financial projections will use tier-based conversion probabilities: Tier 1 (70-85%), Tier 2 (50-70%), Tier 3 (20-40%), Tier 4 (0-15% - deprioritize)

**Context**:
- Research Task 2 scored 55 tools on visibility (web presence, social, reviews) and sales maturity (pricing clarity, demo, trial)
- Clear tiers emerged: YC recent grads + well-funded seed (Tier 1), niche specialists + UK/EU geographic (Tier 2), early-stage/low visibility (Tier 3), MVP/pre-PMF (Tier 4)
- Original plan assumed uniform 60-70% conversion across all vendors
- Tier-based modeling enables more accurate revenue forecasting and outreach prioritization

**Rationale**:
- **Accurate forecasting**: Tier 1 vendors (YC S23-W25, £3M+ seed) convert at 70-85% vs Tier 4 (MVP stage) at 0-15%
- **Outreach efficiency**: Focus 80% of time on Tier 1, 15% on Tier 2, 5% on Tier 3, skip Tier 4
- **Success criteria alignment**: Month 3 target becomes "4-5 Tier 1 + 2-3 Tier 2" instead of generic "5-8 vendors"
- **Risk management**: Conservative path assumes lower Tier 1 count, aggressive path assumes higher

**Financial Model Paths**:
- **Conservative**: 10 Tier 1 @ 70%, 8 Tier 2 @ 50%, 5 Tier 3 @ 20% = £2,277 MRR (23 vendors)
- **Recommended**: 12 Tier 1 @ 75%, 10 Tier 2 @ 60%, 8 Tier 3 @ 30% = £3,465 MRR (30 vendors)
- **Aggressive**: 15 Tier 1 @ 85%, 12 Tier 2 @ 70%, 10 Tier 3 @ 40% = £5,346 MRR (37 vendors)

**Alternatives Considered**:
- **Uniform conversion rate**: Rejected - ignores reality of vendor tiers, leads to inaccurate forecasting
- **Binary model** (high/low only): Rejected - oversimplifies, misses niche specialists (Tier 2)
- **Five tiers**: Rejected - operational complexity, diminishing returns on granularity

**Expected Outcome**:
- Month 3: 4-5 Tier 1, 2-3 Tier 2 vendors signed
- Month 6: 8-10 Tier 1, 5-7 Tier 2, 2-3 Tier 3
- Month 12: 10-15 Tier 1, 8-12 Tier 2, 5-10 Tier 3, <5 Tier 4
- Validate conversion probabilities by Month 6 (adjust if Tier 1 actual <70%)

**Trade-offs Accepted**:
- More complex tracking (must classify each vendor by tier)
- Some subjectivity in tier assignment (acceptable - use clear criteria)
- Tier 3 vendors get less attention despite needing more help (acceptable - resource constraints)

**Actual Outcome** (Review Date: 2026-03-13):
[To be filled during monthly review]

**Impact Level**: High
**Tags**: strategy, business-model, vendor, financial-modeling, ICP

---

### DEC-017: Five Competitive Advantages Framework (2025-12-13)

**Decision**: IndustryLabs competitive positioning centers on five core differentiators: (1) AI-Native Only, (2) Request Board Matching, (3) Curated 60-75 Tools, (4) Compliance + Integration Depth, (5) Vertical-Specific Workflows

**Context**:
- Research Task 3 analyzed competitive landscape: G2/Capterra (incumbent directories), FutureTools/Futurepedia (AI aggregators), HR Tech Analysts (Josh Bersin), Niche Communities
- Conclusion: Massive white space - no direct competitor owns "AI-native only B2B tools for People Ops"
- Need clear articulation of why IndustryLabs is different (not just "another directory")

**Rationale**:
- **AI-Native Only**: Excludes legacy SaaS (BambooHR, Workday), focuses on 2020+ founded tools with AI core
- **Request Board as Matching Engine**: Self-serve lead gen (qualified requests → vendor intros) vs passive directory listings
- **Curated 60-75 Tools**: Quality over quantity - rigorous vetting vs 200+ mediocre listings or 5,000+ AI tool dumps
- **Compliance + Integration Depth**: GDPR, EEOC, SOC2 verification + 30 platform integrations (Workday, BambooHR, Greenhouse) vs generic "check our website"
- **Vertical-Specific Workflows**: HR recruiting pipelines, performance cycles, engagement programs vs generic "productivity tools" or "AI for business"

**Competitive Gaps Exploited**:
- G2/Capterra: Mix legacy + AI-native, generic horizontal reviews, low-quality lead gen
- FutureTools/Futurepedia: Consumer-focused, breadth over depth, no compliance/integration tracking
- HR Tech Analysts: £10K+ consulting engagements, paywalled reports, not self-serve
- Niche Communities: Fragmented, anecdotal, no structured data

**Alternatives Considered**:
- **Single differentiator focus** ("We're the Request Board"): Rejected - too narrow, competitors can copy
- **Ten differentiators**: Rejected - messaging dilution, hard to remember
- **Generic positioning** ("Better B2B directory"): Rejected - not defensible

**Expected Outcome**:
- Vendor pitch leads with: "We're the only AI-native only marketplace with Request Board matching..."
- Buyer pitch: "Unlike G2 (legacy + AI-native mix), we exclusively list AI-first tools with verified compliance..."
- Content strategy: Each differentiator becomes article series (comparison guides, explainers)
- Competitive moat: Five differentiators together create barrier (competitors can copy one, not all five)

**Trade-offs Accepted**:
- More complex positioning (vs simple "B2B tool marketplace")
- Must defend each differentiator with evidence (operational overhead)
- Some buyers might not care about all five (acceptable - targeting mid-market who do care)

**Actual Outcome** (Review Date: 2026-03-13):
[To be filled during monthly review]

**Impact Level**: High
**Tags**: strategy, positioning, competitive-moat, differentiation, go-to-market

---

### DEC-018: Tier-Based Vendor Outreach Sequencing (2025-12-13)

**Decision**: Vendor outreach will follow tier-based sequencing: Month 1-3 focus 80% on Tier 1 (YC recent grads, well-funded seed), 15% on Tier 2 (niche specialists, UK/EU geographic), 5% on Tier 3 (early-stage, low visibility), skip Tier 4 (MVP, pre-PMF) until Month 9+

**Context**:
- DEC-016 established tier-based conversion probabilities: Tier 1 (70-85%), Tier 2 (50-70%), Tier 3 (20-40%), Tier 4 (0-15%)
- Limited founder time (30-36 hrs/week) requires prioritization
- Need to hit Month 3 targets: £500-1,000 MRR, 5-8 paying vendors

**Rationale**:
- **Efficiency**: Tier 1 vendors convert at 70-85% vs Tier 3 at 20-40% - same outreach effort, 3-4x higher conversion
- **Social proof acceleration**: Landing 2-3 Tier 1 vendors (recognizable brands) creates credibility for subsequent outreach
- **Resource allocation**: First 10 vendors require 3-5 hours each - better to spend on high-conversion prospects
- **Tier 2 strategic value**: Niche specialists + UK/EU geographic vendors offer unique value despite lower conversion
- **Tier 4 timing**: Pre-PMF vendors not ready to pay £99/mo - revisit Month 9+ when they mature

**Outreach Allocation**:
- **Month 1-3**: 80% Tier 1 (target 10-15 outreach → 7-12 conversions), 15% Tier 2 (target 5-8 → 3-5 conversions), 5% Tier 3 opportunistic
- **Month 4-6**: 60% Tier 1, 30% Tier 2, 10% Tier 3
- **Month 7-12**: 40% Tier 1, 40% Tier 2, 15% Tier 3, 5% Tier 4 selective

**Tier-Specific Templates** (See DEC-019 and RESOURCES update):
- **Template 1A-YC**: YC recent grads (S23-W25) - emphasize network effects, investor portfolio
- **Template 1B-SEED**: Well-funded seed (£3M+ raised) - emphasize growth stage fit
- **Template 1C-NICHE**: Niche specialists - emphasize targeted buyer match quality
- **Template 1D-GEO**: UK/EU geographic - emphasize local market credibility, GDPR positioning

**Alternatives Considered**:
- **Equal outreach across all tiers**: Rejected - wastes time on low-conversion prospects
- **Tier 1 only**: Rejected - limits vendor diversity, misses niche specialists
- **Bottom-up** (Tier 4 first): Rejected - lowest conversion, highest churn risk

**Expected Outcome**:
- Month 3: 4-5 Tier 1 vendors signed (from 6-8 outreach), 2-3 Tier 2 vendors
- Month 6: 8-10 Tier 1 total, 5-7 Tier 2 total
- Tier 1 social proof accelerates Tier 2-3 conversion rates (upward pressure on 50-70% and 20-40%)
- Tier 4 vendors approach us (inbound) by Month 9+ as platform gains credibility

**Trade-offs Accepted**:
- Tier 3-4 vendors get less attention despite potentially needing more support
- Some great Tier 3 tools might be missed early (acceptable - can add later)
- Tier-based approach requires classification overhead (acceptable - worth the efficiency gain)

**Actual Outcome** (Review Date: 2026-03-13):
[To be filled during monthly review]

**Impact Level**: Medium
**Tags**: vendor, go-to-market, workflow, efficiency, ICP

---

### DEC-019: Request Board as Primary Differentiator (Over Directory Depth) (2025-12-13)

**Decision**: Marketing, positioning, and vendor pitch will lead with Request Board (qualified lead matching) as primary value prop, not directory depth/comprehensiveness

**Context**:
- DEC-008 (Nov 2025) established Request Board as Day 1 core feature
- Competitive research (Dec 2025) revealed: G2/Capterra offer passive listings (no buyer-vendor matching), FutureTools/Futurepedia are discovery-only
- Request Board submissions create data flywheel: buyer intent → content insights → better matching → more vendors → more buyers
- Risk: Vendors might view IndustryLabs as "small directory" unless Request Board value is clear

**Rationale**:
- **Unique value**: Request Board is defensible (network effects, intent data), directory depth is commodity
- **Vendor ROI**: Qualified leads with context (budget, timeline, company size, pain points) vs anonymous clicks
- **Buyer value**: Self-serve matching ("describe your needs, get 2-3 pre-vetted intros") vs browsing 75 tools
- **Data moat**: Request submissions reveal buyer patterns → informs editorial curation, vendor prioritization, content strategy
- **Pricing justification**: £99/mo makes sense for lead gen, harder to justify for passive listing in 60-75 tool directory

**Messaging Framework**:
- **Vendor pitch**: "Unlike G2 where vendors pay for passive listings and anonymous clicks, we send you qualified leads with full context: company size, budget, timeline, current stack, pain points. Request Board submissions go directly to you."
- **Buyer pitch**: "Describe your needs once, get introductions to 2-3 pre-vetted vendors who fit your requirements. No 40-vendor comparison spreadsheets."
- **Content strategy**: Feature Request Board prominently on homepage, tool pages, category pages. Not buried in footer.

**Request Board as Competitive Moat**:
- **Month 1-6**: Manual matching (Nelson reviews, selects 2-3 vendors, provides intros) - high quality, slow
- **Month 6-12**: Semi-automated (Airtable formulas suggest matches based on budget/region/compliance, Nelson reviews)
- **Year 2**: AI-powered matching using RECOMMENDATIONS table, buyer intent graph

**Alternatives Considered**:
- **Directory depth first**: Rejected - commoditizes marketplace, easier for competitors to replicate
- **Equal weight** (Request Board + Directory): Rejected - dilutes messaging, confuses value prop
- **Request Board only** (no directory): Rejected - buyers need to browse too, matching requires tool database

**Expected Outcome**:
- Vendor conversations lead with: "We generate qualified leads for you, not just passive visibility"
- 10-15 Request Board submissions by Month 3, 30-50 by Month 6, 80-120 by Month 12
- Request Board data informs:
  - Content strategy (write about common pain points)
  - Tool gaps (which use cases need more vendors)
  - Editorial Picks (tools that win buyer requests most often)
- Request Board becomes self-reinforcing: more buyers → better vendor matching → more vendors join → more buyers

**Trade-offs Accepted**:
- Request Board requires operational overhead (manual matching Month 1-6)
- Risk that buyers don't submit requests initially (mitigated by prominent CTA, clear value prop)
- Some vendors might not see value until they receive first lead (acceptable - prove value quickly)

**Actual Outcome** (Review Date: 2026-03-13):
[To be filled during monthly review]

**Impact Level**: High
**Tags**: strategy, product, positioning, differentiation, competitive-moat, request-board

---

## EXECUTION PHASE DECISIONS (Month 1+)

*New decisions will be added here as they're made, in reverse chronological order*

---

## DECISION PATTERNS & LEARNINGS

### Month 1-3 Pattern Recognition

**Review Date**: 2026-02-15

*[Claude will fill this during quarterly review based on your input]*

**High Impact Decisions This Quarter**:
- [List of High Impact decisions made]

**What worked**:
- [Decisions that proved correct]

**What didn't work**:
- [Decisions that needed reversal or adjustment]

**Key lessons**:
- [Insights for future decisions]

---

### Month 4-6 Pattern Recognition

**Review Date**: 2026-05-15

*[To be filled during quarterly review]*

---

### Month 7-12 Pattern Recognition

**Review Date**: 2026-11-15

*[To be filled during quarterly review]*

---

## DECISIONS REVERSED OR MODIFIED

*[This section tracks decisions that were changed and why]*

### Template for Reversed Decisions:

**DEC-XXX REVERSED: [Original Decision] (Reversal Date)**

**Original Decision**: [What was decided]

**Original Impact Level**: High / Medium / Low

**Why Reversed**: [What changed, what data emerged]

**New Decision**: [Current approach]

**New Impact Level**: High / Medium / Low

**Learning**: [What this taught us]

---

## DECISIONS UNDER CONSIDERATION

*[Decisions being actively debated but not yet made]*

### Template for Pending Decisions:

**PENDING-XXX: [Decision Topic]**

**Context**: [What's the situation]

**Options Being Considered**:
- Option A: [Pros and cons]
- Option B: [Pros and cons]

**Potential Impact**: High / Medium / Low

**Data Needed to Decide**: [What info would resolve this]

**Decision Deadline**: [When must this be decided]

**Current Lean**: [Which way you're leaning and why]

---

## CHANGELOG

**Version 1.0** (Nov 16, 2025):
- Initial Decision Log created
- Documented 10 foundational decisions made during planning phase
- Established decision entry template
- Created structure for pattern tracking and reversals

**Version 1.1** (Nov 16, 2025):
- Redesigned for conversation-driven updates
- Added "How This Document Works" section with update workflows
- Added conversation prompts for weekly/monthly/quarterly reviews
- Added Mini Format for tactical decisions
- Added one-line summaries for all foundational decisions
- Clarified format selection guide for Claude
- Added pre-launch section marker for foundational decisions

**Version 1.2** (Nov 16, 2025):
- Added Impact Level field (High/Medium/Low) to both Full and Mini formats
- Added comprehensive Impact Level assignment guide for Claude
- Added filter prompt for viewing High Impact decisions
- Updated Pattern Recognition sections to focus on High Impact decisions
- Added Impact Level to reversed/pending decision templates
- Impact Level auto-assigned by Claude, user can override

**Version 1.3** (Nov 16, 2025):
- **Fixed Impact Level definition overlap** - Clarified High/Medium/Low with clear decision test
- **Improved Mini Format readability** - Changed to multi-line format matching Full Format
- **Added explicit approval workflow** - Claude must wait for "Approved - update the log" before making changes
- **Consolidated Impact Level guides** - Single clear framework instead of duplicate sections
- **Added DEC numbering rules** - Numbers are permanent, never reused or reordered
- **Simplified format selection** - One-line rule: strategy change = Full, execution change = Mini
- **Standardized tag list** - Fixed set of tags to prevent Claude creating random new ones

**Version 1.4** (Dec 13, 2025):
- Added DEC-014 through DEC-019: Strategic research integration decisions
- Incorporated findings from three December 2025 research tasks
- Added tier-based vendor classification and financial modeling framework
- Established five core competitive differentiators
- Updated competitive positioning based on market analysis

---

### When User Requests Decision Capture

**User says**: "Here are decisions I made this week: [descriptions]"

**Your process**:

**Step 1 - Extract and Format** (First response):
1. Extract each decision from user's description
2. Determine format: Strategy/business model/value prop change → Full Format. Execution change → Mini Format
3. Assign Impact Level using the decision test: "If this fails, what needs to change?" (Everything=High, Some things=Medium, Just this=Low)
4. Assign next DEC number:
   - Check highest existing DEC number in document
   - Add 1 to get next number
   - **DEC numbers are permanent** - never reuse or reorder
5. Format each decision using appropriate template
6. Use tags from standard tag list only
7. Show formatted entries to user
8. Explain your reasoning for format and impact level choices
9. **End with**: "Do you approve these entries? If yes, reply 'Approved - update the log' and I'll add them in my next response."

**Step 2 - Update Document** (Second response, only after approval):
1. User says "Approved - update the log" (or similar)
2. Add formatted decisions under "EXECUTION PHASE DECISIONS" section
3. Use reverse chronological order (newest DEC number first)
4. Confirm: "Added DEC-XXX, DEC-YYY to Decision Log."

**Example**:
```
User: "This week I decided to pause paid ads until Month 6,
and I also decided to change pricing to £149/mo instead of £99."

You (First Response):
"I see two decisions:

1. Pause paid ads → Mini Format, Medium Impact
   - This is execution/resource allocation, not strategy change
   - If this fails, you just restart ads - only this one thing changes

2. Pricing change £99 → £149 → Full Format, High Impact
   - This changes core business model and vendor acquisition
   - If this fails, entire monetization approach needs rethinking

Here are the formatted entries:

### DEC-011: Pause paid advertising until Month 6 (2025-11-23)

****Decision****: Pause all paid advertising spend until Month 6 to focus budget on organic channels.

****Why****: Current paid ad ROI is unclear with limited conversion data. Better to invest time in SEO and vendor outreach where we can measure results directly. Will revisit paid ads once we have 10+ Premium vendors and clear conversion metrics.

****Expected Outcome****: £300-500/month saved, redirected to Softr/tooling. No measurable traffic loss since paid ads currently <5% of visitors.

****Review Date****: 2026-05-01
****Actual Outcome****: [To be filled later]

****Impact Level****: Medium
****Tags****: go-to-market, workflow

---

### DEC-012: Increase Premium pricing from £99/mo to £149/mo (2025-11-23)

****Decision****: Raise Premium tier pricing from £99/month to £149/month per vertical, effective for all new vendors starting December 1, 2025. Grandfather existing Early Bird vendors at £99/mo.

****Context****: [You'll need to provide context - why the change? What prompted this?]

****Rationale****: [You'll need to provide rationale]

****Alternatives Considered****:
- Keep £99/mo: [Why rejected?]
- Go to £199/mo: [Why rejected?]

****Expected Outcome****: [What do you hope happens?]

****Actual Outcome**** (Review Date: 2026-02-23): [To be filled later]

****Impact Level****: High
****Tags****: pricing, business-model

---

Do you approve these entries? If yes, reply 'Approved - update the log' and I'll add them in my next response.

Note: For DEC-012, I need more details on context, rationale, alternatives, and expected outcome since this is a High Impact decision requiring Full Format."

---

User (approving): "Approved - update the log"

You (Second Response):
"Added DEC-011 and DEC-012 to the Decision Log under 'EXECUTION PHASE DECISIONS' section."
```

---

### When User Requests Outcome Updates

**User says**: "Update outcomes for decisions needing review"

**Your process**:

**Step 1 - Identify and Query** (First response):
1. Scan document for decisions with "Review Date" in the past
2. List them, prioritizing High Impact first
3. For each one, show: DEC number, title, impact level, original expected outcome
4. Ask: "What actually happened with this decision?"
5. **Do not update document yet**

**Step 2 - Format Updates** (After user provides info):
1. User describes what happened
2. Format the "Actual Outcome" section based on their description
3. Show the updated entry
4. Ask if decision succeeded, failed, or was modified
5. If High Impact decision failed, suggest adding to "Decisions Reversed" section
6. **End with**: "Do you approve these updates? If yes, reply 'Approved - update the log'."

**Step 3 - Update Document** (Next response, after approval):
1. User says "Approved - update the log"
2. Update the "Actual Outcome" sections for reviewed decisions
3. If any decisions reversed, add to "DECISIONS REVERSED OR MODIFIED" section
4. Confirm: "Updated outcomes for DEC-XXX, DEC-YYY."

---

### When User Requests Pattern Analysis

**User says**: "Analyze decision patterns for [time period]"

**Your process**:

**Step 1 - Analyze** (First response):
1. Review all decisions in specified time period
2. **Focus especially on High Impact decisions** - these matter most for learning
3. Group by outcome: successful, failed, modified
4. Identify patterns across decisions (e.g., "High Impact tactical pivots succeeded quickly, High Impact strategic changes took 2x longer than expected")
5. Compare expected vs actual outcomes for High Impact decisions
6. Extract 3-5 key lessons that could inform future decisions
7. Format for appropriate "Pattern Recognition" section
8. Show analysis to user
9. **End with**: "Do you approve this analysis? If yes, reply 'Approved - update the log'."

**Step 2 - Update Document** (Next response, after approval):
1. User says "Approved - update the log"
2. Fill the appropriate "Month X-Y Pattern Recognition" section
3. Confirm: "Added pattern analysis for Month X-Y."

---

### When User Requests High Impact Filter

**User says**: "Show me all High Impact decisions from [period]"

**Your process** (This is a query, not an update - no approval needed):
1. Scan document for "Impact Level: High"
2. Filter by time period if specified
3. For each High Impact decision, extract:
   - DEC number and title
   - Date
   - One-line summary (if available) or first sentence of decision
   - Current status (Expected / Reviewed / Reversed)
   - Outcome if reviewed
   - Tags
4. Group by status:
   - Successful (reviewed, positive outcome)
   - Modified or Reversed
   - Pending Review
5. Present in clean format

**Example output**:
```
HIGH IMPACT DECISIONS (Month 1-3):

Reviewed - Successful:
- DEC-008: Request Board Day 1 (2025-11-16)
  Summary: Launched Request Board as core feature
  Outcome: 12 submissions by Month 3, 85% match rate ✓
  Tags: product, differentiation, strategy

Pending Review (Review date passed):
- DEC-005: £99/mo pricing (2025-11-16)
  Summary: Premium tier at £99/mo per vertical
  Review Date: 2026-02-15 (needs update)
  Tags: pricing, business-model

Modified:
- [None in this period]

Total High Impact Decisions: 10
Reviewed: 1
Pending Review: 9
```

---

### DEC Number Management

**Rules** (enforce strictly):
1. DEC numbers start at 001 and increment sequentially
2. Check highest existing number before assigning new number
3. **DEC numbers are permanent** - never reused even if decision is deleted or reversed
4. If decision is reversed, original DEC keeps its number, reversal gets new DEC number
5. Numbers never reordered - always chronological by assignment date

**Example**:
```
DEC-015: Pricing set to £99/mo (later reversed)
...
DEC-023: Pricing changed to £149/mo (reverses DEC-015)

Both keep their numbers. DEC-015 marked as "REVERSED - See DEC-023"`

---

### Tag Usage

**Always use tags from this standard list**:

- strategy
- product
- pricing
- business-model
- content
- vendor
- technical
- workflow
- go-to-market
- ICP
- positioning
- vertical-expansion
- focus
- speed-to-market
- MVP
- differentiation
- monetization
- early-adopters
- productivity
- AI-assisted
- market
- geography

**If user's decision doesn't fit existing tags**:

1. Use closest match from list
2. Or ask user: "This decision doesn't fit standard tags. Should I create new tag '[proposed-tag]' or use '[closest-match]'?"
3. Only create new tags with explicit user approval

**Never create tags spontaneously.**

---

### Impact Level Assignment - Decision Test

**Use this simple test**: "If this decision fails, how much needs to change?"

**High Impact** = Core business elements need to change:

- Entire strategy needs rethinking
- Business model doesn't work
- Value proposition fails
- Major technical platform needs replacement
- Market position fundamentally wrong

**Examples of High**:

- Pricing model (per-vertical vs flat vs per-tool)
- Target market (mid-market vs SMB vs enterprise)
- Core features (Request Board, Editorial Picks)
- Platform choice (Softr vs custom)
- Geographic focus (UK vs US)

**Medium Impact** = Execution approach in one area needs adjustment:

- Process or workflow needs redesign
- Resource allocation shift required
- Tactical approach change needed
- Tool replacement within existing stack

**Examples of Medium**:

- Content production method (AI vs manual)
- Vendor outreach cadence (3/7/14 days vs 5/10/15 days)
- Social media posting frequency
- Email template style
- Feature build order within roadmap

**Low Impact** = Just this one thing needs tweaking:

- Easy to reverse with minimal cost
- Doesn't affect other decisions
- Operational or cosmetic adjustment

**Examples of Low**:

- Button color, layout tweaks
- Meeting schedule changes
- Minor optimization (image compression)
- Documentation format

**When unsure**:

- Default to Medium (most decisions are Medium)
- Ask user: "Would you consider this High, Medium, or Low impact?"

**User can always override** - if user specifies different impact level, use their judgment without question.

## DL-007 – Dual AI Assistant Stack for Development

**Date:** 2025-11-20

**Owner:** Nelson

**Status:** Active

**Decision:**

IndustryLabs will use a combination of **Claude Code (VS Code extension with Sonnet 4.5)** and **GPT-5 / latest GPT models (ChatGPT, OpenAI Codex)** as the primary AI development stack.

**Context:**

- Claude Code is strongest at in-repo navigation, multi-file edits, and applying refactors safely
- GPT-5 is strongest at high-level reasoning, architecture design, TypeScript type work, and documentation
- Other models (Gemini, [Claude.ai](http://claude.ai/)) can provide additional perspectives for critical paths
- This dual approach balances speed (autonomous coding) with quality (architectural planning)

**Implementation:**

- Phase 1 (PLAN): Use GPT-5 for architecture and task breakdown
- Phase 2 (IMPLEMENT): Use Claude Code for file creation and edits
- Phase 3 (REVIEW): Use GPT-5 for code review and improvements
- Phase 4 (VALIDATE): Optionally use alternative models for critical paths

**Consequences:**

- All dev tasks in Execution Hub should specify: PLAN / IMPLEMENT / REVIEW phase
- Repo remains single source of truth; AI suggestions must be reconciled with actual codebase
- Credentials and API keys are never pasted into AI chats
- Month 1-3: More manual implementation for learning
- Month 4+: More autonomous implementation for speed

**Review Date:** Month 3 (reassess if tools or workflow needs adjustment)

---

# CODE DOCUMENTATION & REFERENCES

**Last Updated:** 2025-11-23

**Foundation Phase:** Complete ✅

This section contains links to generated documentation and key reference files in the codebase.

---

## Generated Documentation Files

These files are version-controlled in the repository at `industrylabs-mvp/docs/`:

### 1. CODEBASE_SNAPSHOT.md

**Location:** `docs/CODEBASE_SNAPSHOT.md`

**Purpose:** Complete code structure reference

**Contents:**

- Full project file tree
- Key directory explanations
- Tech stack listing
- Environment variables reference
- Functional module descriptions
- Data flow diagrams

**Use Cases:**

- Onboarding new developers
- Providing context to AI assistants
- Project structure reference
- Code review preparation

**How to Use:**
Upload this file to ChatGPT/Claude when asking about the codebase for full context.

---

### 2. FOUNDATION_SUMMARY.md

**Location:** `docs/FOUNDATION_SUMMARY.md`

**Purpose:** Foundation phase completion summary

**Contents:**

- All completed tasks (TASK-007 through TASK-305)
- Technical achievements and metrics
- Current production status
- Known issues and technical debt
- Next phase preparation

**Use Cases:**

- Project status reporting
- Milestone documentation
- Historical reference
- Handoff documentation

---

### 3. ENV_SETUP.md

**Location:** `ENV_SETUP.md` (root directory)

**Purpose:** Environment configuration guide

**Contents:**

- Resend API setup (Test Mode vs Production)
- Airtable configuration
- Environment variables explanation
- DNS configuration steps (for production email)

**Use Cases:**

- Setting up local development environment
- Deploying to new Vercel projects
- Troubleshooting environment issues
- Onboarding developers

---

## Key Schema Files

### Airtable Schema Reference

**File:** `AIRTABLE_SCHEMA__DATA_MODEL.pdf`

**Status:** Authoritative source of truth

**Version:** 2.2 (Next.js Integration Ready)

**Important Note:**

Always refer to this document for field names, types, and relationships. The actual implementation may have some deviations (documented in Notion's "AIRTABLE SCHEMA & DATA MODEL" → Implementation Notes).

---

### TypeScript Type Definitions

**Location:** `lib/types/`

```
lib/types/
├── request.ts        # Request Board types (RequestFormPayload, API responses)
└── [future types]

```

**Validation Schemas:**

**Location:** `lib/validation/`

```
lib/validation/
└── request.ts        # Zod schema for Request Board validation

```

---

## Reusable Scripts & Utilities

### Test Airtable Connection

**Location:** `scripts/test-airtable-connection.ts`

**Purpose:** Verify Airtable API credentials and table access

**Usage:**

```bash
npx tsx scripts/test-airtable-connection.ts

```

**Output:**

```
✅ Connection successful!
Found X tools in TOOLS table

First tool sample:
- ID: recXXXXXXXXXX
- Fields: ['tool_name', 'tool_slug', ...]
- Tool name: [name]

```

**When to Use:**

- After environment variable changes
- Troubleshooting Airtable integration
- Verifying new field additions

---

## API Routes Documentation

### POST /api/request

**File:** `app/api/request/route.ts`

**Purpose:** Handle Request Board form submissions

**Request Body:**

```tsx
{
  requesterName: string;
  requesterEmail: string;
  requesterCompany: string;
  companySize: "1-50" | "51-200" | "201-500" | "500+";
  vertical: "HR & Talent" | "Learning & Development" | "Customer Support";
  useCase: string;
  timeline: "Immediate" | "1-3 months" | "3-6 months" | "Exploring";
  requirements: string;
  gdprConsent: true;
  // ... other optional fields
}

```

**Response (Success):**

```tsx
{
  success: true,
  requestId: "recXXXXXXXXXX",
  message: "Request submitted successfully...",
  emailSent: true
}

```

**Response (Error):**

```tsx
{
  success: false,
  error: "Validation failed...",
  fieldErrors?: {
    [fieldName]: "Error message"
  }
}

```

**Features:**

- ✅ Zod validation (client + server)
- ✅ Field-level error messages
- ✅ Writes to Airtable REQUESTS table
- ✅ Sends email notification (non-blocking)
- ✅ Auto-detects source channel
- ✅ Captures request source URL

---

## Component Library Reference

### UI Components (shadcn/ui)

**Location:** `components/ui/`

**Installed Components:**

```
alert          - Success/error messages
badge          - Category tags, labels
button         - Primary actions
card           - Content containers
checkbox       - Form checkboxes
input          - Text inputs
label          - Form labels
select         - Dropdown selects
separator      - Visual dividers
textarea       - Multi-line text inputs

```

**Adding New Components:**

```bash
npx shadcn@latest add [component-name]

```

**Documentation:** [https://ui.shadcn.com/docs/components](https://ui.shadcn.com/docs/components)

---

### Custom Components

**Layout Components:**

```
components/layout/
├── header.tsx          - Main navigation
├── footer.tsx          - Site footer with links
└── container.tsx       - Max-width wrapper

```

**Feature Components:**

```
components/home/        - Homepage-specific components
components/tools/       - Tool-related components (ToolCard)
components/hr/          - HR category page components
components/navigation/  - Breadcrumbs, etc.
components/request/     - Request Board form

```

---

## Helper Functions Reference

### lib/airtable-helpers.ts

**Available Functions:**

```tsx
// Tools
getAllTools(): Promise<Tool[]>
getFeaturedTools(limit?: number): Promise<Tool[]>
getToolsByVertical(vertical: string): Promise<Tool[]>
getToolBySlug(slug: string): Promise<Tool | null>

// Articles
getAllArticles(): Promise<Article[]>
getArticleBySlug(slug: string): Promise<Article | null>
getRelatedToolsForArticle(article: Article): Promise<Tool[]>

```

**Usage Example:**

```tsx
// In a Server Component
import { getAllTools } from "@/lib/airtable-helpers";

export default async function ToolsPage() {
  const tools = await getAllTools();
  return <ToolsGrid tools={tools} />;
}

```

---

### lib/email.ts

**Available Functions:**

```tsx
sendNewRequestEmail(params: {
  requestId: string;
  values: RequestFormValues;
}): Promise<void>

```

**Email Features:**

- HTML and plain text versions
- Formatted with all request details
- Includes request ID for Airtable lookup
- Shows use case and requirements separately
- GB timezone formatting

---

## Code Templates & Patterns

### Creating a New Page with Airtable Data

**Template:**

```tsx
// app/new-page/page.tsx
import { Container } from "@/components/layout/container";
import { getAllTools } from "@/lib/airtable-helpers";
import { mockTools } from "@/lib/mock-data";

export default async function NewPage() {
  // Fetch data with fallback
  let tools;
  try {
    tools = await getAllTools();
    if (tools.length === 0) tools = mockTools;
  } catch (error) {
    console.error("Error fetching tools:", error);
    tools = mockTools;
  }

  return (
    <Container className="py-12">
      <h1>Page Title</h1>
      {/* Your content */}
    </Container>
  );
}

```

---

### Creating a New API Route

**Template:**

```tsx
// app/api/new-endpoint/route.ts
import { NextRequest, NextResponse } from "next/server";
import { z } from "zod";
import { createRecord } from "@/lib/airtable";

// Define validation schema
const schema = z.object({
  field: z.string().min(1),
});

export async function POST(req: NextRequest) {
  try {
    // Parse and validate
    const body = await req.json();
    const result = schema.safeParse(body);

    if (!result.success) {
      return NextResponse.json(
        { success: false, error: "Validation failed" },
        { status: 400 }
      );
    }

    // Write to Airtable
    const record = await createRecord("TABLE_NAME", result.data);

    return NextResponse.json(
      { success: true, id: record.id },
      { status: 201 }
    );
  } catch (error) {
    console.error("Error:", error);
    return NextResponse.json(
      { success: false, error: "Server error" },
      { status: 500 }
    );
  }
}

```

---

### Creating a New Form Component

**Template:**

```tsx
"use client";

import { useState, FormEvent } from "react";
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import { Label } from "@/components/ui/label";

export function MyForm() {
  const [values, setValues] = useState({ field: "" });
  const [isSubmitting, setIsSubmitting] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [success, setSuccess] = useState(false);

  const handleSubmit = async (e: FormEvent) => {
    e.preventDefault();
    setError(null);
    setIsSubmitting(true);

    try {
      const res = await fetch("/api/endpoint", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(values),
      });

      const data = await res.json();

      if (!res.ok || !data.success) {
        setError(data.error || "Failed");
        return;
      }

      setSuccess(true);
      setValues({ field: "" }); // Reset
    } catch (err) {
      setError("Network error");
    } finally {
      setIsSubmitting(false);
    }
  };

  return (
    <form onSubmit={handleSubmit}>
      <div>
        <Label htmlFor="field">Field</Label>
        <Input
          id="field"
          value={values.field}
          onChange={(e) => setValues({ ...values, field: e.target.value })}
          disabled={isSubmitting}
        />
      </div>

      {error && <p className="text-red-600">{error}</p>}
      {success && <p className="text-green-600">Success!</p>}

      <Button type="submit" disabled={isSubmitting}>
        {isSubmitting ? "Submitting..." : "Submit"}
      </Button>
    </form>
  );
}

```

---

## Development Workflow

### Local Development Setup

**Prerequisites:**

- Node.js 18+ installed
- Git repository cloned
- Environment variables configured

**Steps:**

1. **Install dependencies:**

```bash
   npm install

```

1. **Configure environment:**

```bash
   cp .env.example .env.local
   # Edit .env.local with your API keys

```

1. **Run development server:**

```bash
   npm run dev

```

1. **Open browser:**

```
   <http://localhost:3000>

```

---

### Testing Checklist

**Before committing code:**

- [ ]  Run `npm run build` (ensures TypeScript compiles)
- [ ]  Test on mobile viewport (responsive design)
- [ ]  Verify all forms submit successfully
- [ ]  Check Airtable for new records
- [ ]  Test email notifications (if applicable)
- [ ]  Review console for errors/warnings
- [ ]  Test with network throttling (slow 3G)

---

### Deployment Workflow

**Current Setup:**

- **Platform:** Vercel
- **Branch:** `main` (auto-deploys)
- **URL:** [https://industrylabs.vercel.app](https://industrylabs.vercel.app/)

**Manual Deployment:**

1. **Commit and push:**

```bash
   git add .
   git commit -m "feat: description"
   git push origin main

```

1. **Vercel auto-deploys:**
    - Build takes ~2-3 minutes
    - Automatic preview for PRs
    - Production deployment on `main`
2. **Verify deployment:**
    - Check Vercel dashboard for build logs
    - Visit production URL
    - Test critical user flows

---

## Troubleshooting Resources

### Common Issues & Solutions

**Issue: "Cannot find module '@/lib/...'"**

- Solution: Check `tsconfig.json` paths configuration
- Verify file exists at expected location

**Issue: Airtable "Unauthorized" error**

- Solution: Verify `AIRTABLE_API_KEY` in environment variables
- Check API key has correct permissions

**Issue: Email not sending**

- Solution: Verify `RESEND_API_KEY` is set
- Check email is verified in Resend dashboard (Test Mode)
- Check Vercel logs for error details

**Issue: Build fails with TypeScript errors**

- Solution: Run `npm run build` locally to see full errors
- Fix type mismatches
- Ensure all imports are correct

**Issue: Page shows mock data instead of Airtable data**

- Solution: Check console for Airtable errors
- Verify table names are exactly correct (case-sensitive)
- Test Airtable connection with test script

---

## External Resources

### Official Documentation

- **Next.js:** [https://nextjs.org/docs](https://nextjs.org/docs)
- **TypeScript:** [https://www.typescriptlang.org/docs](https://www.typescriptlang.org/docs)
- **Tailwind CSS:** [https://tailwindcss.com/docs](https://tailwindcss.com/docs)
- **shadcn/ui:** [https://ui.shadcn.com](https://ui.shadcn.com/)
- **Airtable API:** [https://airtable.com/developers/web/api/introduction](https://airtable.com/developers/web/api/introduction)
- **Resend API:** [https://resend.com/docs](https://resend.com/docs)
- **Zod:** [https://zod.dev](https://zod.dev/)

### Learning Resources

**Next.js App Router:**

- [https://nextjs.org/learn](https://nextjs.org/learn)

**TypeScript Best Practices:**

- [https://www.typescriptlang.org/docs/handbook/intro.html](https://www.typescriptlang.org/docs/handbook/intro.html)

**React Server Components:**

- [https://react.dev/reference/rsc/server-components](https://react.dev/reference/rsc/server-components)

---

## Version Control

### Branch Strategy

- `main` - Production branch (protected)
- Feature branches: `feature/task-xxx-description`
- Hotfixes: `hotfix/description`

### Commit Convention

**Format:** `type: description`

**Types:**

- `feat:` - New feature
- `fix:` - Bug fix
- `docs:` - Documentation changes
- `style:` - Code style changes (formatting)
- `refactor:` - Code refactoring
- `test:` - Adding tests
- `chore:` - Maintenance tasks

**Examples:**

```
feat: add Request Board email notifications
fix: resolve TypeScript build errors in request form
docs: update CODEBASE_SNAPSHOT with new components
refactor: optimize Airtable data fetching

```

---

## Contact & Support

### Getting Help

**For code-related questions:**

- Review this documentation
- Check `docs/CODEBASE_SNAPSHOT.md`
- Search codebase for examples

**For Airtable schema questions:**

- Refer to `AIRTABLE_SCHEMA__DATA_MODEL.pdf`
- Check Notion "AIRTABLE SCHEMA & DATA MODEL" for implementation notes

**For deployment issues:**

- Check Vercel dashboard logs
- Review environment variables
- Consult `ENV_SETUP.md`

---

## Quick Links

| Resource | Location | Purpose |
| --- | --- | --- |
| Codebase Snapshot | `docs/CODEBASE_SNAPSHOT.md` | Full code structure |
| Foundation Summary | `docs/FOUNDATION_SUMMARY.md` | Project milestones |
| Environment Setup | `ENV_SETUP.md` | Configuration guide |
| Airtable Schema | `AIRTABLE_SCHEMA__DATA_MODEL.pdf` | Data model reference |
| Production Site | [https://industrylabs.vercel.app](https://industrylabs.vercel.app/) | Live website |
| Vercel Dashboard | [https://vercel.com/dashboard](https://vercel.com/dashboard) | Deployment management |
| Airtable Base | [Your Airtable URL] | Database management |
| Resend Dashboard | [https://resend.com/dashboard](https://resend.com/dashboard) | Email management |

---

**Last Updated:** 2025-11-23

**Maintained By:** IndustryLabs Development Team

**Next Review:** When starting Growth Phase

---

**End of Resources Documentation**